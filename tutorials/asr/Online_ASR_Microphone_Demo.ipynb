{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Please run notebook locally (if you have all the dependencies and a GPU). \n",
    "Technically you can run this notebook on Google Colab but you need to set up microphone for Colab.\n",
    "\n",
    "Instructions for setting up Colab are as follows:\n",
    "1. Open a new Python 3 notebook.\n",
    "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
    "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
    "4. Run this cell to set up dependencies.\n",
    "5. Set up microphone for Colab\n",
    "\"\"\"\n",
    "# If you're using Google Colab and not running locally, run this cell.\n",
    "\n",
    "## Install dependencies\n",
    "#!pip install wget\n",
    "#!apt-get install sox libsndfile1 ffmpeg portaudio19-dev\n",
    "#!pip install unidecode\n",
    "#!pip install pyaudio\n",
    "\n",
    "# ## Install NeMo\n",
    "#BRANCH = 'main'\n",
    "#!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[asr]\n",
    "\n",
    "## Install TorchAudio\n",
    "#!pip install torchaudio>=0.6.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "## Grab the config we'll use in this example\n",
    "#!mkdir configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates automatic speech recognition (ASR) from a microphone's stream in NeMo.\n",
    "\n",
    "It is **not a recommended** way to do inference in production workflows. If you are interested in \n",
    "production-level inference using NeMo ASR models, please refer to NVIDIA RIVA: https://developer.nvidia.com/riva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook requires PyAudio library to get a signal from an audio device.\n",
    "For Ubuntu, please run the following commands to install it:\n",
    "```\n",
    "sudo apt-get install -y portaudio19-dev\n",
    "pip install pyaudio\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires the `torchaudio` library to be installed for MatchboxNet. Please follow the instructions available at the [torchaudio Github page](https://github.com/pytorch/audio#installation) to install the appropriate version of torchaudio.\n",
    "\n",
    "If you would like to install the latest version, please run the following command to install it:\n",
    "\n",
    "```\n",
    "conda install -c pytorch torchaudio\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyaudio as pa\n",
    "import os, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\pytorch_lightning\\metrics\\__init__.py:43: LightningDeprecationWarning: `pytorch_lightning.metrics.*` module has been renamed to `torchmetrics.*` and split off to its own package (https://github.com/PyTorchLightning/metrics) since v1.3 and will be removed in v1.5\n",
      "  rank_zero_deprecation(\n",
      "[NeMo W 2021-09-25 00:22:20 optimizers:47] Apex was not found. Using the lamb optimizer will error out.\n",
      "[NeMo W 2021-09-25 00:22:23 nemo_logging:349] D:\\anaconda3\\lib\\site-packages\\torchaudio\\extension\\extension.py:14: UserWarning: torchaudio C++ extension is not available.\n",
      "      warnings.warn('torchaudio C++ extension is not available.')\n",
      "    \n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2021-09-25 00:22:23 nemo_logging:349] D:\\anaconda3\\lib\\site-packages\\torchaudio\\backend\\utils.py:63: UserWarning: The interface of \"soundfile\" backend is planned to change in 0.8.0 to match that of \"sox_io\" backend and the current interface will be removed in 0.9.0. To use the new interface, do `torchaudio.USE_SOUNDFILE_LEGACY_INTERFACE = False` before setting the backend to \"soundfile\". Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2021-09-25 00:22:26 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali.AudioToCharDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# sample rate, Hz\n",
    "SAMPLE_RATE = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-09-25 00:22:28 modelPT:138] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: train/golos_and_mcv.jsonl\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - ' '\n",
      "    - а\n",
      "    - б\n",
      "    - в\n",
      "    - г\n",
      "    - д\n",
      "    - е\n",
      "    - ж\n",
      "    - з\n",
      "    - и\n",
      "    - й\n",
      "    - к\n",
      "    - л\n",
      "    - м\n",
      "    - н\n",
      "    - о\n",
      "    - п\n",
      "    - р\n",
      "    - с\n",
      "    - т\n",
      "    - у\n",
      "    - ф\n",
      "    - х\n",
      "    - ц\n",
      "    - ч\n",
      "    - ш\n",
      "    - щ\n",
      "    - ъ\n",
      "    - ы\n",
      "    - ь\n",
      "    - э\n",
      "    - ю\n",
      "    - я\n",
      "    batch_size: 64\n",
      "    trim_silence: false\n",
      "    max_duration: 20.0\n",
      "    min_duration: 0.1\n",
      "    num_workers: 20\n",
      "    shuffle: true\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    tarred_shard_strategy: scatter\n",
      "    parser: ru\n",
      "    \n",
      "[NeMo W 2021-09-25 00:22:28 modelPT:145] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath:\n",
      "    - test/mcv/dev_ru.jsonl\n",
      "    - test/mcv/test_ru.jsonl\n",
      "    - test/crowd/crowd.jsonl\n",
      "    - test/farfield/farfield.jsonl\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - ' '\n",
      "    - а\n",
      "    - б\n",
      "    - в\n",
      "    - г\n",
      "    - д\n",
      "    - е\n",
      "    - ж\n",
      "    - з\n",
      "    - и\n",
      "    - й\n",
      "    - к\n",
      "    - л\n",
      "    - м\n",
      "    - н\n",
      "    - о\n",
      "    - п\n",
      "    - р\n",
      "    - с\n",
      "    - т\n",
      "    - у\n",
      "    - ф\n",
      "    - х\n",
      "    - ц\n",
      "    - ч\n",
      "    - ш\n",
      "    - щ\n",
      "    - ъ\n",
      "    - ы\n",
      "    - ь\n",
      "    - э\n",
      "    - ю\n",
      "    - я\n",
      "    batch_size: 64\n",
      "    num_workers: 20\n",
      "    shuffle: false\n",
      "    parser: ru\n",
      "    \n",
      "[NeMo W 2021-09-25 00:22:28 modelPT:151] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: test/mcv/test_ru.jsonl\n",
      "    sample_rate: 16000\n",
      "    labels:\n",
      "    - ' '\n",
      "    - а\n",
      "    - б\n",
      "    - в\n",
      "    - г\n",
      "    - д\n",
      "    - е\n",
      "    - ж\n",
      "    - з\n",
      "    - и\n",
      "    - й\n",
      "    - к\n",
      "    - л\n",
      "    - м\n",
      "    - н\n",
      "    - о\n",
      "    - п\n",
      "    - р\n",
      "    - с\n",
      "    - т\n",
      "    - у\n",
      "    - ф\n",
      "    - х\n",
      "    - ц\n",
      "    - ч\n",
      "    - ш\n",
      "    - щ\n",
      "    - ъ\n",
      "    - ы\n",
      "    - ь\n",
      "    - э\n",
      "    - ю\n",
      "    - я\n",
      "    batch_size: 64\n",
      "    shuffle: false\n",
      "    parser: ru\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-09-25 00:22:28 features:252] PADDING: 16\n",
      "[NeMo I 2021-09-25 00:22:28 features:269] STFT using torch\n",
      "[NeMo I 2021-09-25 00:22:30 modelPT:438] Model EncDecCTCModel was successfully restored from D:\\work\\am\\QuartzNet15x5_golos.nemo.\n"
     ]
    }
   ],
   "source": [
    "asr_model = nemo_asr.models.EncDecCTCModel.restore_from(r'D:\\work\\am\\QuartzNet15x5_golos.nemo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the model from NGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#asr_model = nemo_asr.models.EncDecCTCModel.from_pretrained('QuartzNet15x5Base-En')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing the config of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preserve a copy of the full config\n",
    "cfg = copy.deepcopy(asr_model._cfg)\n",
    "#print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify preprocessor parameters for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# # Make config overwrite-able\n",
    "# OmegaConf.set_struct(cfg.preprocessor, False)\n",
    "\n",
    "# # some changes for streaming scenario\n",
    "# cfg.preprocessor.dither = 0.0\n",
    "# cfg.preprocessor.pad_to = 0\n",
    "\n",
    "# # spectrogram normalization constants\n",
    "# normalization = {}\n",
    "# normalization['fixed_mean'] = [\n",
    "#      -14.95827016, -12.71798736, -11.76067913, -10.83311182,\n",
    "#      -10.6746914,  -10.15163465, -10.05378331, -9.53918999,\n",
    "#      -9.41858904,  -9.23382904,  -9.46470918,  -9.56037,\n",
    "#      -9.57434245,  -9.47498732,  -9.7635205,   -10.08113074,\n",
    "#      -10.05454561, -9.81112681,  -9.68673603,  -9.83652977,\n",
    "#      -9.90046248,  -9.85404766,  -9.92560366,  -9.95440354,\n",
    "#      -10.17162966, -9.90102482,  -9.47471025,  -9.54416855,\n",
    "#      -10.07109475, -9.98249912,  -9.74359465,  -9.55632283,\n",
    "#      -9.23399915,  -9.36487649,  -9.81791084,  -9.56799225,\n",
    "#      -9.70630899,  -9.85148006,  -9.8594418,   -10.01378735,\n",
    "#      -9.98505315,  -9.62016094,  -10.342285,   -10.41070709,\n",
    "#      -10.10687659, -10.14536695, -10.30828702, -10.23542833,\n",
    "#      -10.88546868, -11.31723646, -11.46087382, -11.54877829,\n",
    "#      -11.62400934, -11.92190509, -12.14063815, -11.65130117,\n",
    "#      -11.58308531, -12.22214663, -12.42927197, -12.58039805,\n",
    "#      -13.10098969, -13.14345864, -13.31835645, -14.47345634]\n",
    "# normalization['fixed_std'] = [\n",
    "#      3.81402054, 4.12647781, 4.05007065, 3.87790987,\n",
    "#      3.74721178, 3.68377423, 3.69344,    3.54001005,\n",
    "#      3.59530412, 3.63752368, 3.62826417, 3.56488469,\n",
    "#      3.53740577, 3.68313898, 3.67138151, 3.55707266,\n",
    "#      3.54919572, 3.55721289, 3.56723346, 3.46029304,\n",
    "#      3.44119672, 3.49030548, 3.39328435, 3.28244406,\n",
    "#      3.28001423, 3.26744937, 3.46692348, 3.35378948,\n",
    "#      2.96330901, 2.97663111, 3.04575148, 2.89717604,\n",
    "#      2.95659301, 2.90181116, 2.7111687,  2.93041291,\n",
    "#      2.86647897, 2.73473181, 2.71495654, 2.75543763,\n",
    "#      2.79174615, 2.96076456, 2.57376336, 2.68789782,\n",
    "#      2.90930817, 2.90412004, 2.76187531, 2.89905006,\n",
    "#      2.65896173, 2.81032176, 2.87769857, 2.84665271,\n",
    "#      2.80863137, 2.80707634, 2.83752184, 3.01914511,\n",
    "#      2.92046439, 2.78461139, 2.90034605, 2.94599508,\n",
    "#      2.99099718, 3.0167554,  3.04649716, 2.94116777]\n",
    "\n",
    "# cfg.preprocessor.normalize = normalization\n",
    "\n",
    "# # Disable config overwriting\n",
    "# OmegaConf.set_struct(cfg.preprocessor, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup preprocessor with these settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#asr_model.preprocessor = asr_model.from_config_dict(cfg.preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Set model to inference mode\n",
    "asr_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "asr_model = asr_model.to(asr_model.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up data for Streaming Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from nemo.core.classes import IterableDataset\n",
    "from nemo.core.neural_types import NeuralType, AudioSignal, LengthsType\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# simple data layer to pass audio signal\n",
    "class AudioDataLayer(IterableDataset):\n",
    "    @property\n",
    "    def output_types(self):\n",
    "        return {\n",
    "            'audio_signal': NeuralType(('B', 'T'), AudioSignal(freq=self._sample_rate)),\n",
    "            'a_sig_length': NeuralType(tuple('B'), LengthsType()),\n",
    "        }\n",
    "\n",
    "    def __init__(self, sample_rate):\n",
    "        super().__init__()\n",
    "        self._sample_rate = sample_rate\n",
    "        self.output = True\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if not self.output:\n",
    "            raise StopIteration\n",
    "        self.output = False\n",
    "        return torch.as_tensor(self.signal, dtype=torch.float32), \\\n",
    "               torch.as_tensor(self.signal_shape, dtype=torch.int64)\n",
    "        \n",
    "    def set_signal(self, signal):\n",
    "        self.signal = signal.astype(np.float32)/32768.\n",
    "        self.signal_shape = self.signal.size\n",
    "        self.output = True\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "data_layer = AudioDataLayer(sample_rate=16000)\n",
    "data_loader = DataLoader(data_layer, batch_size=1, collate_fn=data_layer.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# inference method for audio signal (single instance)\n",
    "def infer_signal(model, signal):\n",
    "    data_layer.set_signal(signal)\n",
    "    batch = next(iter(data_loader))\n",
    "    audio_signal, audio_signal_len = batch\n",
    "    audio_signal, audio_signal_len = audio_signal.to(asr_model.device), audio_signal_len.to(asr_model.device)\n",
    "    log_probs, encoded_len, predictions = model.forward(\n",
    "        input_signal=audio_signal, input_signal_length=audio_signal_len\n",
    "    )\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from demo_decoder import CTCBeamSearchDecoder\n",
    "\n",
    "# class for streaming frame-based ASR\n",
    "# 1) use reset() method to reset FrameASR's state\n",
    "# 2) call transcribe(frame) to do ASR on\n",
    "#    contiguous signal's frames\n",
    "class FrameASR:\n",
    "    \n",
    "    def __init__(self, model_definition,\n",
    "                 kenlm_model,\n",
    "                 frame_len=2, frame_overlap=2.5, \n",
    "                 show_fixed_part=True,\n",
    "                 offset=10):\n",
    "        '''\n",
    "        Args:\n",
    "          frame_len: frame's duration, seconds\n",
    "          frame_overlap: duration of overlaps before and after current frame, seconds\n",
    "          offset: number of symbols to drop for smooth streaming\n",
    "        '''\n",
    "        self.vocab = list(model_definition['labels'])\n",
    "        self.vocab.append('_')\n",
    "        \n",
    "        self.sr = model_definition['sample_rate']\n",
    "        self.frame_len = frame_len\n",
    "        self.n_frame_len = int(frame_len * self.sr)\n",
    "        self.frame_overlap = frame_overlap\n",
    "        self.n_frame_overlap = int(frame_overlap * self.sr)\n",
    "        timestep_duration = model_definition['AudioToMelSpectrogramPreprocessor']['window_stride']\n",
    "        for block in model_definition['JasperEncoder']['jasper']:\n",
    "            timestep_duration *= block['stride'][0] ** block['repeat']\n",
    "        self.n_timesteps_overlap = int(frame_overlap / timestep_duration) - 2\n",
    "        self.buffer = np.zeros(shape=2*self.n_frame_overlap + self.n_frame_len,\n",
    "                               dtype=np.float32)\n",
    "        self.offset = offset\n",
    "        self.reset()\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = CTCBeamSearchDecoder(kenlm_model, self.vocab,\n",
    "                                            beam_size=15, loglike_gap=3.0,\n",
    "                                            acoustic_loglike_gap=2.0,\n",
    "                                            detach_fixed_part=True)\n",
    "        self.decoder.reset()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _decode(self, frame, offset=0):\n",
    "        #print(frame, len(frame))\n",
    "        assert len(frame) == self.n_frame_len\n",
    "        self.buffer[:-self.n_frame_len] = self.buffer[self.n_frame_len:]\n",
    "        self.buffer[-self.n_frame_len:] = frame\n",
    "        logits = infer_signal(asr_model, self.buffer).cpu().numpy()[0]\n",
    "        #decoded = self._greedy_decoder(\n",
    "        #    logits[self.n_timesteps_overlap:-self.n_timesteps_overlap],\n",
    "        #    self.vocab\n",
    "        #)\n",
    "        for frame_logits in logits:\n",
    "            self.decoder.process_frame(frame_logits)\n",
    "        return self.decoder.tree()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def transcribe(self, frame=None, merge=True):\n",
    "        if frame is None:\n",
    "            frame = np.zeros(shape=self.n_frame_len, dtype=np.float32)\n",
    "        if len(frame) < self.n_frame_len:\n",
    "            frame = np.pad(frame, [0, self.n_frame_len - len(frame)], 'constant')\n",
    "        unmerged = self._decode(frame, self.offset)\n",
    "        if not merge:\n",
    "            return unmerged\n",
    "        return self.greedy_merge(unmerged)\n",
    "    \n",
    "    def reset(self):\n",
    "        '''\n",
    "        Reset frame_history and decoder's state\n",
    "        '''\n",
    "        self.buffer=np.zeros(shape=self.buffer.shape, dtype=np.float32)\n",
    "        self.prev_char = ''\n",
    "        \n",
    "    def best_hyp(self):\n",
    "        return self.decoder.best_hyp()\n",
    "\n",
    "    @staticmethod\n",
    "    def _greedy_decoder(logits, vocab):\n",
    "        s = ''\n",
    "        for i in range(logits.shape[0]):\n",
    "            s += vocab[np.argmax(logits[i])]\n",
    "        return s\n",
    "\n",
    "    def greedy_merge(self, s):\n",
    "        s_merged = ''\n",
    "        \n",
    "        for i in range(len(s)):\n",
    "            if s[i] != self.prev_char:\n",
    "                self.prev_char = s[i]\n",
    "                if self.prev_char != '_':\n",
    "                    s_merged += self.prev_char\n",
    "        return s_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Inference\n",
    "\n",
    "Streaming inference depends on a few factors, such as the frame length and buffer size. Experiment with a few values to see their effects in the below cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "\n",
    "# duration of signal frame, seconds\n",
    "FRAME_LEN = 1.0\n",
    "# number of audio channels (expect mono signal)\n",
    "CHANNELS = 1\n",
    "\n",
    "import kenlm\n",
    "lm_golos = kenlm.Model(r'D:\\work\\am\\lm_golos.binary')\n",
    "\n",
    "CHUNK_SIZE = int(FRAME_LEN*SAMPLE_RATE)\n",
    "asr = FrameASR(model_definition = {\n",
    "                   'sample_rate': SAMPLE_RATE,\n",
    "                   'AudioToMelSpectrogramPreprocessor': cfg.preprocessor,\n",
    "                   'JasperEncoder': cfg.encoder,\n",
    "                   'labels': cfg.decoder.vocabulary\n",
    "               },\n",
    "               kenlm_model=lm_golos,\n",
    "               frame_len=FRAME_LEN, frame_overlap=1,\n",
    "               offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Microsoft Sound Mapper - Input\n",
      "1 Headset (FreeBuds 3 Hands-Free \n",
      "2 Microphone (Realtek(R) Audio)\n",
      "7 Stereo Mix (Realtek HD Audio Stereo input)\n",
      "10 PC Speaker (Realtek HD Audio output with HAP)\n",
      "11 Microphone (Realtek HD Audio Mic input)\n",
      "15 Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\n",
      ";(FreeBuds 3))\n",
      "17 Input (@System32\\drivers\\bthhfenum.sys,#4;%1 Hands-Free HF Audio%0\n",
      ";(HUAWEI MediaPad M5 lite 10))\n",
      "18 Line ()\n",
      "20 Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\n",
      ";(HUAWEI CM510))\n",
      "22 Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\n",
      ";(HUAWEI CM510))\n",
      "Please type input device ID:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-09-25 00:22:35 patch_utils:49] torch.stft() signature has been updated for PyTorch 1.7+\n",
      "    Please update PyTorch to remain compatible with later versions of NeMo.\n",
      "[NeMo W 2021-09-25 00:22:35 nemo_logging:349] D:\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "    To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "      return torch.floor_divide(self, other)\n",
      "    \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19568/1274496546.py\u001b[0m in \u001b[0;36mcallback\u001b[1;34m(in_data, frame_count, time_info, status)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mglobal\u001b[0m \u001b[0mempty_counter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0msignal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_hyp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19568/3014646380.py\u001b[0m in \u001b[0;36m_decode\u001b[1;34m(self, frame, offset)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mframe_logits\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe_logits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\work\\NeMo\\tutorials\\asr\\demo_decoder.py\u001b[0m in \u001b[0;36mtree\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m             \u001b[0mtokens\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mhypothesis_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfixed_part_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\work\\NeMo\\tutorials\\asr\\demo_decoder.py\u001b[0m in \u001b[0;36mhypothesis_tree\u001b[1;34m(tokens, compress)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0madj_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m                 \u001b[0madj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madj_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0madj_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PyAudio stopped\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19568/1274496546.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_active\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# asr.reset()\n",
    "\n",
    "p = pa.PyAudio()\n",
    "#print('Available audio input devices:')\n",
    "input_devices = []\n",
    "for i in range(p.get_device_count()):\n",
    "    dev = p.get_device_info_by_index(i)\n",
    "    if dev.get('maxInputChannels'):\n",
    "        input_devices.append(i)\n",
    "        print(i, dev.get('name'))\n",
    "\n",
    "if len(input_devices):\n",
    "    dev_idx = -2\n",
    "    while dev_idx not in input_devices:\n",
    "        print('Please type input device ID:')\n",
    "        dev_idx = int(input())\n",
    "\n",
    "    empty_counter = 0\n",
    "\n",
    "    def callback(in_data, frame_count, time_info, status):\n",
    "        global empty_counter\n",
    "        signal = np.frombuffer(in_data, dtype=np.int16)\n",
    "        tree = asr._decode(signal)\n",
    "        clear_output(wait=True)\n",
    "        display(asr.best_hyp())\n",
    "        return (in_data, pa.paContinue)\n",
    "\n",
    "    stream = p.open(format=pa.paInt16,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=SAMPLE_RATE,\n",
    "                    input=True,\n",
    "                    input_device_index=dev_idx,\n",
    "                    stream_callback=callback,\n",
    "                    frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "    print('Listening...')\n",
    "\n",
    "    stream.start_stream()\n",
    "    \n",
    "    # Interrupt kernel and then speak for a few more words to exit the pyaudio loop !\n",
    "    try:\n",
    "        while stream.is_active():\n",
    "            time.sleep(0.1)\n",
    "    finally:        \n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "\n",
    "        print()\n",
    "        print(\"PyAudio stopped\")\n",
    "    \n",
    "else:\n",
    "    print('ERROR: No audio input device found.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
